package com.besimgurbuz.paralleldataprocessing;


import java.util.stream.LongStream;

/**
 * @author Besim Gurbuz
 */
public class UsingParallelStreamsCorrectly {
    /*
    The main cause of errors generated by misuses of parallel streams is the use of algorithms that
    mutate some shared state. Here's a way to implement the sum of the first 'n' natural numbers by
    mutating a shared accumulator:
     */
    public long sideEffectSum(long n) {
        Accumulator accumulator = new Accumulator();
        LongStream.rangeClosed(1, n).forEach(accumulator::add);
        return accumulator.total;
    }

    /*
    It's quite common to write this sort of code, especially for developers who are familiar with
    imperative programming paradigms. This code closely resembles what you're used to doing when
    iterating imperatively a list of numbers: you initialize an accumulator and traverse the elements
    in the list one by one, adding them on the accumulator.

    What's wrong with this code? Unfortunately, it's irretrievably broken because it's fundamentally
    sequential. You have a data race on every access of 'total'. And if you try to fix that with
    synchronization, you'll lose all you parallelism. To understand this, let's try to run the stream
    into a parallel one:
     */

    public long sideEffectParallelSum(long n) {
        Accumulator accumulator = new Accumulator();
        LongStream.rangeClosed(1, n).parallel().forEach(accumulator::add);
        return accumulator.total;
    }

    /*
    Try to run this last method with the harness, also printing the result of each execution:
        System.out.println("SideEffect parallel sum done in: " +
            measurePerf(UsingParallelStreamsCorrectly::sideEffectParallelSum, 10_000_000L) + " msecs");

    You could obtain something like the following:
    Result: 5959989000692
    Result: 7425264100768
    Result: 6827235020033
    Result: 7192970417739
    Result: 6714157975331
    Result: 7497810541907
    Result: 6435348440385
    Result: 6999349840672
    Result: 7435914379978
    Result: 7715125932481
    SideEffect parallel sum done in: 49 msecs

    This time the performance of you method isn't important. The only relevant thing is that each
    execution returns a different result, all distant from the correct value of 50000005000000. This is
    cause by the fact that multiple threads are concurrently accessing the accumulator and, in
    particular, executing total += value, which, despite its appearance, isn't an atomic operation. The
    origin of the problem is that the method invoked inside the forEach block has the side effect of
    changing the mutable state of an object shared among multiple threads. It's mandatory to avoid these
    kinds of situations if you want to use parallel streams without incurring similar bad surprises.

    Now you know that a shared mutable state doesn’t play well with parallel streams and with parallel
    computations in general. We’ll come back to this idea of avoiding mutation in chapters 18 and 19
    when discussing functional programming in more detail. For now,keep in mind that avoiding a shared
    mutable state ensures that your parallel stream will produce the right result. Next, we’ll look at
    some practical advice you can use to figure out when it’s appropriate to use parallel streams to
    gain performance.
     */

    // Using Parallel Streams effectively
    /*
    In general, it's impossible (and pointless) to try to give quantitative hint on when to use a
    parallel stream, because any specific criterion such as "only when stream contains moore than a
    thousand elements" could be correct for specific operation running on a specific machine, but
    completely wrong in a marginally different context. Nonetheless, it's at least possible to provide
    some qualitative advice that could be useful when deciding whether it makes sense to use a parallel
    stream in certain situation:

        * If in doubt, measure. Turning a sequential stream into a parallel one is trivial but not
        always the right thing to do. As we already demonstrated in this section, a parallel stream
        isn't always faster than the corresponding sequential version. Moreover, parallel streams can
        sometimes work in a counterintuitive way, so the first and most important suggestion when
        choosing between sequential and parallel streams is to always check their performance with an
        appropriate benchmark.

        * Be aware for Boxing. If possible use primitive streams for fixing this (IntStream,
        LongStream, DoubleStream)

        * Check operations natural performance on parallel. For example some operations rely on the
        order of elements and that expensive in parallel streams. (ex. limit and findFirst) Unordered
        collections would perform better on parallel. If order is not important we can always turn a
        ordered collection to unordered version by invoking the method `unordered` on it.

        * Consider the total computational cost of the pipeline of operations performed by the stream.
        With N being the number of elements to be processed an Q the approximate cost of processing
        one of these elements through the stream pipeline, the product of N*Q gives a rough
        qualitative estimation of this cost. A higher value for Q implies a better chance of good
        performance when using a parallel stream.

        * For a small amount of data, choosing a parallel stream is almost never a winning decision.
        Because advantages of processing in parallel only few elements not always worth to compute in
        parallel

        * Consider how well the data structure underlying the stream decomposes. For instance an
        ArrayList can be split much more efficiently than LinkedList, because the first can be evenly
        divided without traversing it, as it's necessary to do with the second. (We can get full
        control of this decomposition process by implementing our own Spliterator)

        * The characteristics of a stream, and how the intermediate operations through the pipeline
        modify them, can change the performance of the decomposition process, For example, a SIZED
        stream can be divided into two equal parts, and then each part can be processed in parallel
        more effectively, but a filter operation can throw away an unpredictable number of elements,
        making the size of the stream itself unknown.

        * Consider whether a terminal operation has a cheap or expensive merge step (for example, the
        combiner method in a Collector). If this is expensive, then the cost caused by the combination
        of the partial results generated by each sub-stream can outweigh the performance benefits of a
        parallel stream.


       Parallel-Friendliness

            SOURCE          |           DECOMPOSABLE
        ArrayList                       Excellent
        LinkedList                      Poor
        IntStream.range                 Excellent
        Stream.iterate                  Poor
        HashSet                         Good
        TreeSet                         Good

      Finally, we need to emphasize that the infrastructure used behind the scenes by parallel Streams
      to execute operations in parallel is the fork/join framework. Let's learn fork/join!
     */
}

class Accumulator {
    public long total = 0;
    public void add(long value) { total +=value; }
}
